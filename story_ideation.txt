Fixed Pipeline for Consistent Story Generation
This pipeline is designed to generate a story with consistent characters using multiple AI models. The process starts with user input, proceeds through a structured conversation between two characters, includes periodic summarization and narration triggers, and ends with a master summary. The output is saved in both JSON and Markdown formats.

Pipeline Steps:
User Input: The user provides rough story ideas and specifies the total number of utterances (N) and the summarization interval (x).

Prompt Refinement: Model A refines the rough input into a detailed prompt.

Opening Scene: Model A generates an opening scene based on the refined prompt.

Conversation Flow: Character models (Char1 and Char2) alternate utterances for N times, with summarization every x utterances using Model S.

Narration Trigger: After each summarization, Model C (a binary classifier) decides whether to trigger Model D for adding engaging narration.

Master Summary: After all utterances, Model B generates a master summary from all segment summaries.

Output: All data is compiled into JSON and Markdown files.

Detailed Pseudocode Implementation
python
# Import necessary libraries (example for Python)
import json

# Assume all models are pre-loaded or accessible via API
# model_A, model_Char1, model_Char2, model_S, model_C, model_D, model_B

# User inputs
rough_data = "Rough story idea from user"  # Example: "A fantasy adventure about a knight and a rogue"
total_utterances = 20  # Total number of utterances (N)
summary_interval = 4   # Summarize every x utterances (x)

# Step 1: Refine prompt using Model A
refined_prompt = model_A(f"Create a detailed story prompt from this rough idea: {rough_data}")

# Step 2: Generate opening scene using Model A
opening_scene = model_A(f"{refined_prompt} Write an engaging opening scene for the story.")

# Initialize data storage
data = {
    "initial_context": refined_prompt,
    "opening_scene": opening_scene,
    "conversations": [],  # List of utterances: each is {"speaker": "Char1" or "Char2", "text": "..."}
    "summaries": [],      # List of segment summaries from Model S
    "narrations": [],     # List of narrations from Model D
    "master_summary": ""  # Will be filled later
}

# Set initial context for conversation
current_context = opening_scene
last_speaker = None  # Track the last speaker to alternate turns

# Step 3: Start conversation with Char1
first_prompt = f"{current_context} What does Character1 say first?"
char1_response = model_Char1(first_prompt)
data['conversations'].append({"speaker": "Char1", "text": char1_response})
current_context += f"\nChar1: {char1_response}"
last_speaker = "Char1"

# Step 4: Continue conversation for remaining utterances
for utterance_index in range(2, total_utterances + 1):
    # Determine next speaker: alternate between Char1 and Char2
    next_speaker = "Char2" if last_speaker == "Char1" else "Char1"
    
    # Generate response from the next speaker
    prompt_char = f"{current_context} How does {next_speaker} respond?"
    if next_speaker == "Char1":
        char_response = model_Char1(prompt_char)
    else:
        char_response = model_Char2(prompt_char)
    
    # Store response and update context
    data['conversations'].append({"speaker": next_speaker, "text": char_response})
    current_context += f"\n{next_speaker}: {char_response}"
    last_speaker = next_speaker

    # Step 5: Summarize after every x utterances
    if utterance_index % summary_interval == 0:
        # Extract the last x utterances for summarization
        recent_conv = data['conversations'][-summary_interval:]
        recent_text = "\n".join([f"{item['speaker']}: {item['text']}" for item in recent_conv])
        
        # Call Model S for summarization
        summary = model_S(f"Summarize this conversation segment:\n{recent_text}")
        data['summaries'].append(summary)
        
        # Step 6: Check if narration is needed using Model C
        trigger_prompt = f"Based on this summary, should we add narration to engage the reader? Answer only True or False. Summary: {summary}"
        trigger_response = model_C(trigger_prompt)
        trigger = trigger_response.strip().lower() == "true"
        
        if trigger:
            # Call Model D for narration
            narration_prompt = f"Based on the current story context and the summary, write a brief engaging narration: Context: {current_context} Summary: {summary}"
            narration = model_D(narration_prompt)
            data['narrations'].append(narration)
            current_context += f"\nNarrator: {narration}"  # Add narration to context

# Step 7: Generate master summary using Model B
all_summaries = "\n".join(data['summaries'])
master_summary = model_B(f"Create a master summary of the entire story from these segment summaries:\n{all_summaries}")
data['master_summary'] = master_summary

# Step 8: Save output to JSON and Markdown
# JSON output
with open('story_data.json', 'w') as json_file:
    json.dump(data, json_file, indent=4)

# Markdown output
md_content = f"""
# Story Generated

## Initial Context
{data['initial_context']}

## Opening Scene
{data['opening_scene']}

## Conversations
{"".join([f"### {conv['speaker']}\n{conv['text']}\n\n" for conv in data['conversations']])}

## Segment Summaries
{"".join([f"### Summary {i+1}\n{summary}\n\n" for i, summary in enumerate(data['summaries'])])}

## Narrations
{"".join([f"### Narration {i+1}\n{narration}\n\n" for i, narration in enumerate(data['narrations'])])}

## Master Summary
{data['master_summary']}
"""

with open('story_output.md', 'w') as md_file:
    md_file.write(md_content)
Explanation:
Models Used:

Model A: Refines the user's rough idea and generates the opening scene.

Char1 and Char2: Generate dialogue for their respective characters, maintaining consistency.

Model S: Summarizes every x utterances to capture key points.

Model C: Acts as a binary classifier to decide if narration is needed.

Model D: Adds engaging narration when triggered.

Model B: Creates a master summary from all segment summaries.

Data Storage: All data is stored in a dictionary (data) that includes initial context, opening scene, conversations, summaries, narrations, and the master summary.

Output Files:

story_data.json: Contains all data in JSON format for structured access.

story_output.md: Presents the story in a human-readable Markdown format.

This pipeline ensures a natural story flow with consistent characters, periodic summarization, and optional narration, all within a fixed number of conversations. The output is well-organized for user review.

Modifications:
Orchestrator role: a dedicated “manager” job that coordinates phases, speaker turns, summarization triggers, narration, and final export.

Phased context state: splitting the story into structured phases (setup, rising action, climax, resolution), each with its own prompt variation, to keep flow natural and avoid bloated context.

User confirmation checkpoint: after Model A generates refined prompt + phase plan, show it to the user for edits/approval before the system runs autonomously.

API key & quota strategy:

Char1/Char2 separated to dodge rate limits.

A/S/D consolidated into one model with role-shifting.

B & C as separate light models.

Model C hardening: acknowledgement that it won’t always behave, so we’d sanitize outputs (regex/post-processor, or sentiment filter) to force True/False compliance.

Resource-aware orchestration: throttle utterances, batch summarizations/narration, cache where possible.


